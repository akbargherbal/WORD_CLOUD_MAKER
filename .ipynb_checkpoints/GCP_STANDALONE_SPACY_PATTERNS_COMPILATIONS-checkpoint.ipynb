{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Current Time = 00:22:07.300033\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def time_now():\n",
    "    '''Get Current Time'''\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S.%f\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    return now\n",
    "\n",
    "\n",
    "############\n",
    "print('Starting...')\n",
    "start = time_now()\n",
    "############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['#', '’', \"'\"  '!',  ',',  '(',  '.',  '‡',  '9',  '*',  '2',  ']',  '†',  '1',  '\"',  '/',  '5',  '4',  '0',  ';',  '¾',  '‘',  '¼',  '{',  ')',  '…',  '½',  '6',  '×',  '[',  '+',  ':',  '$',  '3',  '?',  '7',  '©',  '’',  '£',  '8']\n",
    "\n",
    "pat_verb_noun_phrase =[\n",
    "    {'POS': {'IN': 'VERB ADV'.split()}, 'OP':'+'}, \n",
    "    {'POS': {'IN' : 'DET ADP PART PRON'.split()}, 'OP':'*'},\n",
    "    {'POS': {'IN': 'NOUN ADJ PROPN'.split()}, 'OP':'+'}\n",
    "]\n",
    "\n",
    "\n",
    "pat_noun_verb_phrase = [\n",
    "            {'POS': {'IN': ['NOUN', 'ADJ']}, 'OP': '+'},\n",
    "            {'POS': {'IN': ['AUX', 'PART', 'ADP']}, 'OP': '*'},\n",
    "            {'POS': {'IN': ['VERB', 'ADV']}, 'OP': '+'}\n",
    "            ]\n",
    "            \n",
    "            \n",
    "\n",
    "pat_n_p_v = [\n",
    "            {'POS': {'IN': ['NOUN', 'ADJ', 'PROPN']}, 'OP': '+'},\n",
    "            {'POS': {'IN': ['PUNCT', 'CCONJ']}, 'LEMMA': {'NOT_IN': ignore_list}, 'OP': '+'},\n",
    "            {'POS': {'IN': ['NOUN', 'ADJ', 'VERB', 'PROPN']}, 'OP':'+'}\n",
    "             ]            \n",
    "             \n",
    "pat_noun = [\n",
    "    {'POS': {'IN': 'NOUN ADJ PROPN'.split()} , 'OP':'+'}\n",
    "                        ]\n",
    "\n",
    "pat_n_n = [\n",
    "    {'POS': {'IN': ['NOUN', 'ADJ', 'PROPN']}, 'OP': '+'},\n",
    "    {'POS': {'IN': ['DET', 'ADP', 'PART']}, 'OP': '+'},\n",
    "    {'POS': {'IN': ['NOUN', 'ADJ', 'PROPN']}, 'OP': '+'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos_patterns = [i for i in dir() if i.startswith('pat_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter threhold per pattern per batch:   4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold is 4\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    threshold_pattern = int(input('''\n",
    "Enter threhold per pattern per batch:  '''))\n",
    "    print(f\"\"\"\n",
    "Threshold is {threshold_pattern}\n",
    "    \"\"\")\n",
    "except:\n",
    "    print('Threshold must be an integer number; 2 was chosen for you')\n",
    "    threshold_pattern = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_lemma_pattern(doc, pat_name='anything', pat_collection='list_of_dictionaries'):\n",
    "    '''\n",
    "    Given a spacy doc object; find the span according to the pattern given.\n",
    "    ''' \n",
    "    matcher =  Matcher(vocab = nlp.vocab)\n",
    "    matcher.add(f'{pat_name}', pat_collection, greedy='LONGEST')\n",
    "    \n",
    "    \n",
    "    doc_match = matcher(doc)\n",
    "    \n",
    "    \n",
    "    list_container = []\n",
    "    for match in doc_match:\n",
    "        start = match[1]\n",
    "        end = match[2]\n",
    "        \n",
    "        result_lemma = doc[start:end]\n",
    "        result_lemma = [(i.lemma_ + i.whitespace_) for i in result_lemma] # new!\n",
    "        result_lemma = ''.join(result_lemma).strip()\n",
    "\n",
    "        result_token = doc[start:end]\n",
    "        result_token = [(i.text + i.whitespace_) for i in result_token] # new!\n",
    "        result_token = ''.join(result_token).strip()\n",
    "\n",
    "        result_pos = doc[start:end]\n",
    "        result_pos = [i.pos_ for i in result_pos] # new!\n",
    "        result_pos = ' '.join(result_pos).strip()\n",
    "        \n",
    "        if len(result_lemma.split()) > 1:\n",
    "            list_container.append((result_token, result_lemma, result_pos))\n",
    "\n",
    "    return list_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('IATA_TRANSCRIPTION_PATTERNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk('NLP/'):\n",
    "#     files = sorted(files)\n",
    "#     files_path = [os.path.join(root, file) for file in files if file.endswith('.pkl')]\n",
    "#     files_path = [f'./{file}' for file in files_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = ['EN_STEEL.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Where to save results?\n",
      "Give a name for the directory!\n",
      " NLP_DIR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are in NLP_DIR folder\n"
     ]
    }
   ],
   "source": [
    "nlp_dir = input('''Where to save results?\n",
    "Give a name for the directory!\n",
    "''')\n",
    "Path(nlp_dir).mkdir(parents=True, exist_ok=True)\n",
    "nlp_dir = re.sub(r'[\\/\\.]+', '', nlp_dir)\n",
    "print(f'Results are in {nlp_dir} folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EN_STEEL.pkl Batch 1 of 1\n",
      "Starting NLP\n",
      "Current Time = 00:23:57.653802\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016999483108520508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bc6036fd434688b601381020c84a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 00:24:51.414815\n",
      "Finished NLP\n",
      "Total NLP Duration: 0.8833333333333333 minutes\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016013145446777344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4535095f4e46ecae2b6cbc7ccacf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01601552963256836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 12532,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f46ecf21e54119a1a7c14683b743b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014011621475219727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e559d46db44831af7089a3c904f84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021996021270751953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 4226,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed717631dce4b17bf0114b07c8716ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014017581939697266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1de7edb56c4d5689b65eb59351bad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013999223709106445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 21746,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2be36b59d754ab5bec705434ad0ef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/21746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012997865676879883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7c545d2aa4e4b91198f7896b36be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015997648239135742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 8070,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601daf24a674a0aa62d16b12c3413a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/8070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013000011444091797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 5191,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c21b3dc9c4ff8a0298172be56135e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015002727508544922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 33,
       "postfix": null,
       "prefix": "Pandas Apply",
       "rate": null,
       "total": 12310,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d1e57e020847029c833e034564865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished EN_STEEL.pkl Batch 1 of 1\n"
     ]
    }
   ],
   "source": [
    "for (idx, file) in enumerate(files_path):\n",
    "    print(f'Processing {file} Batch {idx+1} of {len(files_path)}')\n",
    "\n",
    "    df_temp = pd.read_pickle(file)\n",
    "    \n",
    "    \n",
    "    print('Starting NLP')\n",
    "    \n",
    "    start_nlp = time_now()\n",
    "#     print('Renaming TEXT_PUNCT Column!!')\n",
    "#     df_temp['TEXT'] = df_temp['TEXT_PUNCT'] # Remove this line, if necessar!!!\n",
    "    \n",
    "    df_temp['NLP'] = df_temp['TEXT'].swifter.apply(lambda x: nlp(x))\n",
    "    \n",
    "    finish_nlp = time_now()\n",
    "    print('Finished NLP')\n",
    "    duration  = ((finish_nlp - start_nlp).seconds)/60\n",
    "    print(f'Total NLP Duration: {duration} minutes')\n",
    "    \n",
    "    dict_verbs_gen = dict(Counter(list(chain(*df_temp.NLP.apply(lambda x: [i.lemma_ for i in x if i.pos_ == 'VERB'])))).most_common())\n",
    "    dict_nouns_gen = dict(Counter(list(chain(*df_temp.NLP.apply(lambda x: [i.lemma_ for i in x if i.pos_ == 'NOUN'])))).most_common())\n",
    "    dict_adj_gen = dict(Counter(list(chain(*df_temp.NLP.apply(lambda x: [i.lemma_ for i in x if i.pos_ == 'ADJ'])))).most_common())\n",
    "    dict_adv_gen = dict(Counter(list(chain(*df_temp.NLP.apply(lambda x: [i.lemma_ for i in x if i.pos_ == 'ADV'])))).most_common())\n",
    "    \n",
    "    pd.DataFrame(data = dict_adj_gen.items()).rename(columns={0: 'TEXT', 1:'FREQ' }).to_pickle(F'./{nlp_dir}/BATCH_{str(idx+1).zfill(2)}_POS_ADJECTIVES.pkl', protocol=4)\n",
    "    pd.DataFrame(data = dict_adv_gen.items()).rename(columns={0: 'TEXT', 1:'FREQ' }).to_pickle(F'./{nlp_dir}/BATCH_{str(idx+1).zfill(2)}_POS_ADVERBS.pkl', protocol=4)\n",
    "    pd.DataFrame(data = dict_nouns_gen.items()).rename(columns={0: 'TEXT', 1:'FREQ' }).to_pickle(F'./{nlp_dir}/BATCH_{str(idx+1).zfill(2)}_POS_NOUNS.pkl', protocol=4)\n",
    "    pd.DataFrame(data = dict_verbs_gen.items()).rename(columns={0: 'TEXT', 1:'FREQ' }).to_pickle(F'./{nlp_dir}/BATCH_{str(idx+1).zfill(2)}_POS_VERBS.pkl', protocol=4)\n",
    "    \n",
    "    \n",
    "    for pat in list_pos_patterns:\n",
    "        pattern_name = pat.upper()\n",
    "        df_temp[pattern_name] = df_temp['NLP'].swifter.apply(lambda x: get_pos_lemma_pattern(x, f'{pat}', [eval(pat)]))\n",
    "        \n",
    "        list_pat = list(chain(*df_temp[pattern_name]))\n",
    "        df_pattern  = pd.DataFrame(data=list_pat)\n",
    "        df_pattern = df_pattern.rename(columns={0: 'TEXT', 1: 'LEMMA', 2: 'POS'})\n",
    "        \n",
    "        dict_pattern = dict(df_pattern['LEMMA'].value_counts())\n",
    "        dict_pattern = {k:v for k,v in dict_pattern.items() if v >= threshold_pattern}\n",
    "        \n",
    "        df_pattern = df_pattern[df_pattern['LEMMA'].swifter.apply(lambda x: x in dict_pattern)].reset_index(drop=True)\n",
    "        df_pattern.to_pickle(F'./{nlp_dir}/BATCH_{str(idx+1).zfill(2)}_{pattern_name}.pkl', protocol=4)\n",
    "    print(f'Finished {file} Batch {idx+1} of {len(files_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m97"
  },
  "kernelspec": {
   "display_name": "string_similarity",
   "language": "python",
   "name": "string_similarity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
